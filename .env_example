# .env_example
# This file outlines the environment variables that can be used to configure the project.
#
# To set up your environment:
# 1. Create a new file in this same directory named .env (just ".env", no file extension).
# 2. Copy all content from this .env_example file into your new .env file.
# 3. Replace the placeholder values (e.g., "your_together_ai_api_key_here") with your actual credentials or desired settings.
# 4. Crucially, ensure your .env file is listed in your .gitignore to prevent it from being committed to version control.

# --- REQUIRED ENVIRONMENT VARIABLES ---
# These variables are directly read from your .env file by the application.

# Your API key for Together AI.
# Obtain this from your Together AI account dashboard.
TOGETHER_API_KEY="your_together_ai_api_key_here"


# --- OPTIONAL CONFIGURATION OVERRIDES ---
# These variables are commonly placed in .env files for easy configuration.
#
# NOTE: For these settings to take effect from your .env file,
#       you must ensure your `code/config.py` is updated to read them using `os.getenv()`.
#       Currently, some of these may be hardcoded or loaded from `config.yaml` in your project.
#
# Uncomment and set them below to use them.

# --- LLM Settings ---
# The specific LLM model to use (e.g., "mistralai/Mistral-7B-Instruct-v0.2" or "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo")
# LLM_MODEL="mistralai/Mistral-7B-Instruct-v0.2"

# The provider for the LLM (e.g., "together")
# LLM_PROVIDER="together"

# The temperature for text generation (e.g., 0.0 for more deterministic output)
# LLM_TEMPERATURE=0.0

# The maximum number of tokens the LLM can generate in a response
# LLM_MAX_TOKENS=512

# The timeout for LLM API calls in seconds
# LLM_TIMEOUT=30


# --- Embedding Model and Vector Store Settings ---
# The embedding model to use for converting text into numerical vectors
# EMBEDDING_MODEL="sentence-transformers/all-MiniLM-L6-v2"

# The device to run the embedding model on: "cpu" or "cuda" (for GPU)
# EMBEDDING_DEVICE="cpu"

# The local path where ChromaDB will store its persistent data
# VECTOR_DB_PATH="./data/chroma_db"